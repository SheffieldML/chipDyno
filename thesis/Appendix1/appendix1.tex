% ******************************* Thesis Appendix A ****************************
\chapter{Mathematical Background} 

\section{Gaussian Identities}
This appendix aims to make the thesis self contained with a very short reference to the basic mathematical identities we used in this document. 

\subsection{Gaussian Density}
Perhaps Gaussian density is the most common probability density, given by
\begin{equation}\label{eq:Gaussian_density}
	p\left(x|\mu,\sigma^{2}\right)=\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left(-\frac{\left(x-\mu\right)^2}{2\sigma^{2}}\right)\triangleq \mathcal{N}\left(x|\mu,\sigma^{2}\right)
\end{equation} 
where $\mu$ denotes the mean and $\sigma^2$ is the variance of the density.

\subsection{Multivariate Gaussian}
Let's $\textbf{x}$ is an $d$-dimensional multivariate normal random variable, then the probability function is given by
\begin{equation}\label{eq:app_probablity density}
	p\left(\textbf{x}|\boldsymbol{\mu},\boldsymbol{\Sigma}\right)\triangleq \mathcal{N}\left(\textbf{x}|\boldsymbol{\mu},\boldsymbol{\Sigma}\right)= \left(2\pi\right)^{-d/2}|\boldsymbol{\Sigma}|^{-\frac{1}{2}}\exp\left(-\frac{1}{2}\left(\textbf{x}-\boldsymbol{\mu}\right)^{\top}\boldsymbol{\Sigma}^{-1}\left(\textbf{x}-\boldsymbol{\mu}\right)\right)
\end{equation} 
where $\boldsymbol{\mu}\in\mathbb{R}^{d}$ denotes the mean and $\boldsymbol{\Sigma}$ is a symmetric, positive covariance matrix with $\left[d\times d\right]$ dimension.

\subsection{Sum of two Gaussians}
Sum of Gaussian variables is also Gaussian. Let's
\begin{equation}\label{eq:Gaussian_density_i}
 x_i\sim\mathcal{N}\left(\mu_i,\sigma^{2}_i\right)
\end{equation}
then the sum is distributed as
\begin{equation}\label{eq:Gaussian_density_sum}
 \sum^{n}_{i=1} x_i\sim\mathcal{N}\left(\sum^{n}_{i=1}\mu_i,\sum^{n}_{i=1}\sigma^{2}_i\right)
\end{equation}
According to central limit theorem, as sum increases, sum of non-Gaussian with finite variance variables is also Gaussian.

\subsection{Scaling a Gaussians}
Scaling a Gaussian variable is also Gaussian. Let's
\begin{equation}\label{eq:Gaussian_density_sc}
 x\sim\mathcal{N}\left(\mu,\sigma^{2}\right)
\end{equation}
then the scaled density is distributed as
\begin{equation}\label{eq:Gaussian_density_sc}
 wx\sim\mathcal{N}\left(w\mu,w^2\sigma^{2}\right)
\end{equation}
which leads to product of Gaussians. The product of two Gaussian is also Gaussian.

\subsection{Product of two Multivariate Gaussians}
Let's, $\textbf{x}$, $\boldsymbol{\mu}_a$ and $\boldsymbol{\mu}_b$ be the size of $\left[d\times 1 \right]$ and $\boldsymbol{\Sigma}_a$ and $\boldsymbol{\Sigma}_b$ be  $\left[d\times d \right]$ covariance matrices. The product of two multivariate Gaussian distributions is proportional to another multivariate  Gaussian distribution given by

\begin{equation}\label{eq:app_products}
\mathcal{N}\left(\textbf{x}|\boldsymbol{\mu}_a,\boldsymbol{\Sigma}_a\right) \mathcal{N}\left(\textbf{x}|\boldsymbol{\mu}_b,\boldsymbol{\Sigma}_b\right)=
Z\mathcal{N}\left(\textbf{x}|\boldsymbol{\mu}_c,\boldsymbol{\Sigma}_c\right)
\end{equation}
where the covariance is
\begin{equation}\label{eq:app_products_cov}
\boldsymbol{\Sigma}_c = \left(\boldsymbol{\Sigma}_a^{-1}+\boldsymbol{\Sigma}_b^{-1}\right)^{-1}
\end{equation}
and mean is 
\begin{equation}\label{eq:app_products_mean}
\boldsymbol{\mu}_c = \boldsymbol{\Sigma}_c \left(\boldsymbol{\Sigma}_a^{-1}\boldsymbol{\mu}_a+\boldsymbol{\Sigma}_b^{-1}\boldsymbol{\mu}_b\right).
\end{equation}
The normalising constant $Z$ is Gaussian in either $\boldsymbol{\mu}_a$ or $\boldsymbol{\mu}_b$ 
\begin{equation}\label{eq:app_products_const}
z_c=\left(2\pi\right)^{-\frac{d}{2}}|\boldsymbol{\Sigma}_a\boldsymbol{\Sigma}_b\boldsymbol{\Sigma}_c^{-1}|^{-\frac{1}{2}}\exp\left(-\frac{1}{2}\left(\boldsymbol{\mu}_a^\top\boldsymbol{\Sigma}_a^{-1}\boldsymbol{\mu}_a+\boldsymbol{\mu}_b^\top\boldsymbol{\Sigma}_b^{-1}\boldsymbol{\mu}_b-\boldsymbol{\mu}_c^\top\boldsymbol{\Sigma}_c^{-1}\boldsymbol{\mu}_c\right)\right).
\end{equation}

Let's $\textbf{y}$ is a $\left[d\textprime \times 1 \right]$ Gaussian random variable whose mean depends linearly depends on  $\textbf{x}$ where $\boldsymbol{\Sigma}_d$ has the dimension $\left[d\textprime \times d \right]$, and $\boldsymbol{\Sigma}_b$ has the dimension $\left[d\textprime \times d\textprime \right]$. The product of two Gaussian is given by

\begin{equation}\label{eq:app_products_lin}
\mathcal{N}\left(\textbf{x}|\boldsymbol{\mu}_a,\boldsymbol{\Sigma}_a\right) \mathcal{N}\left(\textbf{y}|\boldsymbol{\Sigma}_d\boldsymbol{\mu}_b,\boldsymbol{\Sigma}_b\right) \propto
\mathcal{N}\left(\textbf{x}|\boldsymbol{\mu}_c,\boldsymbol{\Sigma}_c\right)
\end{equation}
The product is proportional to a multivariate normal density with mean 
\begin{equation}\label{eq:app_products_mean_lin}
\boldsymbol{\mu}_c = \boldsymbol{\Sigma}_c \left(\boldsymbol{\Sigma}_a^{-1}\boldsymbol{\mu}_a+\boldsymbol{\Sigma}_d^\top\boldsymbol{\Sigma}_b^{-1}\textbf{y}\right).
\end{equation}
and the covariance is
\begin{equation}\label{eq:app_products_cov_lin}
\boldsymbol{\Sigma}_c = \left(\boldsymbol{\Sigma}_a^{-1}+\boldsymbol{\Sigma}_d^\top\boldsymbol{\Sigma}_b^{-1}\boldsymbol{\Sigma}_d\right)^{-1}.
\end{equation} 


\subsection{Conditional and Marginal Distributions}

Let's $\mathcal{N}\left(\textbf{x}|\boldsymbol{\mu},\boldsymbol{\Sigma}\right)$ is a multivariate Gaussian, partitioned into $\textbf{x}=\left[\textbf{x}_1,\textbf{x}_2\right]^\top$ such that


\begin{equation} \label{eq:jointPro_x1_x2}
p\left(\begin{bmatrix}  \textbf{x}_1 \\\textbf{x}_2 \end{bmatrix} \right) \propto
\mathcal{N}\left( \begin{bmatrix} \textbf{x}_1 \\\textbf{x}_2 \end{bmatrix} \middle|
\begin{bmatrix} \boldsymbol{\mu}_1 \\\boldsymbol{\mu}_2 \end{bmatrix}, \begin{bmatrix} \boldsymbol{\Sigma}_{11} & \boldsymbol{\Sigma}_{12} \\
\boldsymbol{\Sigma}_{21} & \boldsymbol{\Sigma}_{22} \end{bmatrix} \right)
\end{equation}
The marginal distributions are 
\begin{equation}\label{marginal_x1}
p\left(\textbf{x}_2\right)\sim\mathcal{N}\left(\textbf{x}_1|\boldsymbol{\mu}_1,\boldsymbol{\Sigma}_{11}\right)
\end{equation}
and
\begin{equation}\label{marginal_x2}
p\left(\textbf{x}_2\right)\sim\mathcal{N}\left(\textbf{x}_2|\boldsymbol{\mu}_2,\boldsymbol{\Sigma}_{22}\right)
\end{equation}
The conditional distributions are
\begin{equation}\label{cond_x1}
p\left(\textbf{x}_1|\textbf{x}_2\right)\sim\mathcal{N}\left(\textbf{x}_1|\boldsymbol{\mu}_1+\boldsymbol{\Sigma}_{12}\boldsymbol{\Sigma}_{22}^{-1}\left(\textbf{x}_2-\boldsymbol{\mu}_2\right),\boldsymbol{\Sigma}_{11}-\boldsymbol{\Sigma}_{12}\boldsymbol{\Sigma}_{22}^{-1}\boldsymbol{\Sigma}_{21}\right)
\end{equation}
and
\begin{equation}\label{cond_x2}
p\left(\textbf{x}_2|\textbf{x}_1\right)\sim\mathcal{N}\left(\textbf{x}_2|\boldsymbol{\mu}_2+\boldsymbol{\Sigma}_{21}\boldsymbol{\Sigma}_{11}^{-1}\left(\textbf{x}_1-\boldsymbol{\mu}_1\right),\boldsymbol{\Sigma}_{22}-\boldsymbol{\Sigma}_{21}\boldsymbol{\Sigma}_{11}^{-1}\boldsymbol{\Sigma}_{12}\right).
\end{equation}

\subsection{Linear Forms}
Let's $p\left(\textbf{x}\right)\sim\mathcal{N}\left(\textbf{x}|\boldsymbol{\mu},\boldsymbol{\Sigma}\right)$ and $\textbf{y}=\textbf{A}\textbf{x}+\textbf{c}$ then $p\left(\textbf{y}\right)\sim\mathcal{N}\left(\textbf{y}|\textbf{A}\boldsymbol{\mu}+\textbf{c},\textbf{A}\boldsymbol{\Sigma}\textbf{A}^\top\right)$. 

\subsection{Gaussian Integrals}
The probability density function integrates to one (by definition), given by
\begin{equation}\label{eq:gaussian_int}
	\int_{\mathbb{R}^d}\mathcal{N}\left(\textbf{x}|\boldsymbol{\mu},\boldsymbol{\Sigma}\right)\text{d}\textbf{x}=1
\end{equation}

\section{Matrix Analysis}
Let's $\textbf{P}$ and $\textbf{Q}$ be non-singular matrices with $\left[d\times d\right]$ dimensions. The inverse of the product of two matrices can be written in terms of the individual inverses
\begin{equation}\label{eq:indv_invVec}
	\left(\textbf{P}\textbf{Q}\right)^{-1}=\textbf{Q}^{-1}\textbf{P}^{-1}.
\end{equation}
and the product with a scaler $c$ is
\begin{equation}\label{eq:indv_invVec}
	\left(c\textbf{P}\right)^{-1}=c^{-1}\textbf{P}^{-1}. 
\end{equation}
The sum of two matrices with inverses are valid for following identity
\begin{equation}\label{eq:inv_matsum1}
	\textbf{P}^{-1}+\textbf{Q}^{-1}=\textbf{P}^{-1}\left(\textbf{P}+\textbf{Q}\right)\textbf{Q}^{-1}.
\end{equation}
and also for 
\begin{equation}\label{eq:inv_matsum2}
	\left(\textbf{P}^{-1}+\textbf{Q}^{-1}\right)^{-1}=\textbf{P}\left(\textbf{P}+\textbf{Q}\right)^{-1}\textbf{Q}=\textbf{Q}\left(\textbf{P}+\textbf{Q}\right)^{-1}\textbf{P}.
\end{equation}

The Woodbury formula, or the matrix inversion lemma  or the Sherman–Morrison–Woodbury formula is given by
\begin{equation}\label{eq:Woodbury}
	\left(\textbf{P}+\textbf{Q}\textbf{R}^{-1}\textbf{S}\right)^{-1}=\textbf{P}^{-1}-\textbf{P}^{-1}\textbf{R}\left(\textbf{Q}+\textbf{S}\textbf{P}^{-1}\textbf{R}\right)^{-1}\textbf{S}\textbf{P}^{-1}.
\end{equation}

\section{Singular Value Decomposition}
The singular value decomposition is a matrix factorization technique of a real or complex matrix. Let's the matrix $\textbf{S}$ is an $\left|m\times n\right|$ real matrix with $m \textgreater n$, then by singular value decomposition $\textbf{S}$ can be written as 
\begin{equation}\label{eq:SVD}
 	\textbf{S}=\textbf{U}\boldsymbol{\Lambda}\textbf{V}^\top
\end{equation}
where $\textbf{U}$ is an orthonormal matrix (i.e.\ $\textbf{U}^\top\textbf{U}=\textbf{I}$) with $\left|m\times n\right|$ dimensions,  $\boldsymbol{\Lambda}$ is the diagonal matrix containing the eigenvalues of $\textbf{S}$ with $\left|n\times n\right|$ dimensions and $\textbf{V}$ is another orthonormal matrix (i.e.\ $\textbf{V}^\top\textbf{V}=\textbf{I}$) with $\left|n\times n\right|$ dimensions. When applied to a positive semi-definite matrix, the singular value decomposition is equivalent to the \emph{eigendecomposition}.
%ref https://www.utdallas.edu/~herve/Abdi-SVD2007-pretty.pdf

\section{Markov Property}
Let's  $\{ X(t), t \geq 0 \}$ be a stochastic time continuous process with non-negative integer values. This process is termed as a discrete Markov process if for every $n\geq 0$, time points  $0 \leq t_{0} < t_{1} < \cdot\cdot\cdot < t_{n} < t_{n+1}$ and states $i_0, i_1,..., i_{n+1}$ it holds that 
\begin{equation}\label{eq:markovProp}
\begin{split}
    & p(X(t_{n+1})=i_{n+1} \mid X(t_{n})=i_{n},X(t_{n-1})=i_{n-1},...,X(t_{0})=i_{0} \\
    & =p(X(t_{n+1})=i_{n+1} \mid X(t_{n}=i_{n})).
\end{split}    
\end{equation}

This definition states that any information of the future behaviour of the process solely depends on the present state. Adding the history of the process does not increase or update any new information.

\section{Cholesky Decompositions}
Inversion of a symmetric positive definite matrix is a very common requirement while working with Gaussian processes and their approximations. Let's $\boldsymbol{\Sigma}$ the symmetric positive definite covariance matrix. In practice we rarely need the $\boldsymbol{\Sigma}^{-1}$ itself. For a given vector $\textbf{y}$ common forms we require are $\left|\boldsymbol{\Sigma}\right|$,  $\textbf{y}^\top\boldsymbol{\Sigma}^{-1}\textbf{y}$, and $\boldsymbol{\Sigma}^{-1}\textbf{y}$. The most efficient and computationally stable way to obtain these forms is via Cholesky decomposition (also known as the matrix square root)
\begin{equation}\label{eq:Cholesky}
 \boldsymbol{\Sigma}=\textbf{L}\textbf{L}^\top
\end{equation}
where $\textbf{L}$ is a triangular matrix known as Cholesky factor. Though Cholesky factor has the same order cost as matrix inversion $\mathcal{O}\left(N^3\right)$, it is cheaper in terms of constant factors.
\begin{equation}\label{eq:Cholesky_dec}
\begin{split}
 \textbf{y}\top\boldsymbol{\Sigma}^{-1}\textbf{y}
    &=\textbf{y}\top\left(\textbf{L}\textbf{L}^\top\right)^{-1}\textbf{y}\\ 
    &=\textbf{y}\top\textbf{L}^{-\top}\textbf{L}^{-1}\textbf{y}\\ 
    &=\parallel\textbf{L}^{-1}\textbf{y}\parallel^2
 \end{split}
\end{equation}
The vector $\textbf{L}^{-1}\textbf{y}$ can be computed with the computational cost $\mathcal{O}\left(N^2\right)$ by forward substitution as $\textbf{L}$ is triangular. $\boldsymbol{\Sigma}^{-1}\textbf{y}$ can be computed using back substitution $\textbf{L}^{-\top}\textbf{L}^{-1}\textbf{y}$ and $\left|\boldsymbol{\Sigma}\right|$ is computed as $\left|\boldsymbol{\Sigma}\right|=\prod_n \textbf{L}^{2}_nn$.

% \section{ GP property}
% \section{ SVD}
% \section{ Markov property}
% \section{ ChipDyno ?}
 

